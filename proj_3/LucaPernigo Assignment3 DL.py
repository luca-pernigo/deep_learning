# -*- coding: utf-8 -*-
"""LucaPernigo Assignment3 DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Szl8t6ioFvIWM2WjKYmNrLZXX0g36-X

# Helper Code for Assignment 3 (RNN language models)

## Reading raw text file & Create DataLoader
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

!nvidia-smi

class Vocabulary:

    def __init__(self, pad_token="<pad>", unk_token='<unk>'):
        self.id_to_string = {}
        self.string_to_id = {}
        
        #1.2.4
        # add the default pad token
        self.id_to_string[0] = pad_token
        self.string_to_id[pad_token] = 0
        
        # add the default unknown token
        self.id_to_string[1] = unk_token
        self.string_to_id[unk_token] = 1        
        
        # shortcut access
        self.pad_id = 0
        self.unk_id = 1
        
    def __len__(self):
        return len(self.id_to_string)

    def add_new_word(self, string):
        self.string_to_id[string] = len(self.string_to_id)
        self.id_to_string[len(self.id_to_string)] = string

    #1.2.1
    # Given a string, return ID
    def get_idx(self, string, extend_vocab=False):
        if string in self.string_to_id:
            return self.string_to_id[string]
        elif extend_vocab:  # add the new word
            self.add_new_word(string)
            return self.string_to_id[string]
        else:
            return self.unk_id


# Read the raw txt file and generate a 1D PyTorch tensor
# containing the whole text mapped to sequence of token IDs, and a vocab object.
class TextData:

    def __init__(self, file_path, vocab=None, extend_vocab=True, device='cuda'):
        self.data, self.vocab = self.text_to_data(file_path, vocab, extend_vocab, device)
        
    def __len__(self):
        return len(self.data)

    def text_to_data(self, text_file, vocab, extend_vocab, device):
        """Read a raw text file and create its tensor and the vocab.

        Args:
          text_file: a path to a raw text file.
          vocab: a Vocab object
          extend_vocab: bool, if True extend the vocab
          device: device

        Returns:
          Tensor representing the input text, vocab file

        """
        assert os.path.exists(text_file)
        if vocab is None:
            vocab = Vocabulary()

        data_list = []

        # Construct data
        full_text = []
        print(f"Reading text file from: {text_file}")
        with open(text_file, 'r') as text:
            for line in text:
                tokens = list(line)
                for token in tokens:
                    # get index will extend the vocab if the input
                    # token is not yet part of the text.
                    full_text.append(vocab.get_idx(token, extend_vocab=extend_vocab))

        # convert to tensor
        data = torch.tensor(full_text, device=device, dtype=torch.int64)
        print("Done.")

        #1.2.3
        #print(len(data))
        return data, vocab
    

# Since there is no need for schuffling the data, we just have to split
# the text data according to the batch size and bptt length.
# The input to be fed to the model will be batch[:-1]
# The target to be used for the loss will be batch[1:]
class DataBatches:

    def __init__(self, data, bsz, bptt_len, pad_id):
        self.batches = self.create_batch(data, bsz, bptt_len, pad_id)

    def __len__(self):
        return len(self.batches)

    def __getitem__(self, idx):
        return self.batches[idx]

    def create_batch(self, input_data, bsz, bptt_len, pad_id):
        """Create batches from a TextData object .

        Args:
          input_data: a TextData object.
          bsz: int, batch size
          bptt_len: int, bptt length
          pad_id: int, ID of the padding token

        Returns:
          List of tensors representing batches

        """
        batches = []  # each element in `batches` is (len, B) tensor
        text_len = len(input_data)
        segment_len = text_len // bsz + 1

        # Question: Explain the next two lines!
        padded = input_data.data.new_full((segment_len * bsz,), pad_id)
        padded[:text_len] = input_data.data
        padded = padded.view(bsz, segment_len).t()
        num_batches = segment_len // bptt_len + 1

        for i in range(num_batches):
            # Prepare batches such that the last symbol of the current batch
            # is the first symbol of the next batch.
            if i == 0:
                # Append a dummy start symbol using pad token
                batch = torch.cat(
                    [padded.new_full((1, bsz), pad_id),
                     padded[i * bptt_len:(i + 1) * bptt_len]], dim=0)
                
                #1.2.6
                #print(padded[i * bptt_len:(i + 1) * bptt_len].shape)
                batches.append(batch)
            else:
                batches.append(padded[i * bptt_len - 1:(i + 1) * bptt_len])

                #1.2.7
                #print(padded[i * bptt_len - 1:(i + 1) * bptt_len].shape)
        #1.2.4
        #print(len(batches))

        
        
        return batches

# downlaod the text
# Make sure to go to the link and check how the text looks like.
!wget http://www.gutenberg.org/files/49010/49010-0.txt

!wget https://raw.githubusercontent.com/luca-pernigo/em/main/jimi-hendrix.txt


!wget https://raw.githubusercontent.com/shivaverma/TV-Script-Generator/master/data/simpsons/moes_tavern_lines.txt

# This is for Colab. Adapt the path if needed.
text_path = "/content/49010-0.txt"

#3.5.1
#text_path = "/content/jimi-hendrix.txt"

#3.5.2
#text_path= "/content/moes_tavern_lines.txt"

#1.1  
# to get info in the terminal-> wc fables.txt

DEVICE = 'cuda'

batch_size = 32
bptt_len = 64

my_data = TextData(text_path, device=DEVICE)
batches = DataBatches(my_data, batch_size, bptt_len, pad_id=0)

"""## Model"""

# RNN based language model
class RNNModel(nn.Module):

    def __init__(self, num_classes, emb_dim, hidden_dim, num_layers):
        """Parameters:
        
          num_classes (int): number of input/output classes
          emb_dim (int): token embedding size
          hidden_dim (int): hidden layer size of RNNs
          num_layers (int): number of RNN layers
        """
        super().__init__()
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.input_layer = nn.Embedding(num_classes, emb_dim)
        self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers)
        self.out_layer = nn.Linear(hidden_dim, num_classes)

    def forward(self, input, state):
        emb = self.input_layer(input)
        output, state = self.rnn(emb, state)
        output = self.out_layer(output)
        output = output.view(-1, self.num_classes)
        return output, state

    def init_hidden(self, bsz):
        weight = next(self.parameters())
        return weight.new_zeros(self.num_layers, bsz, self.hidden_dim)


# To be modified for LSTM...
def custom_detach(h):
    return h.detach()

"""## Decoding"""

@torch.no_grad()
def complete(model, prompt, steps, sample=False):
    """Complete the prompt for as long as given steps using the model.
    
    Parameters:
      model: language model
      prompt (str): text segment to be completed
      steps (int): number of decoding steps.
      sample (bool): If True, sample from the model. Otherwise greedy.

    Returns:
      completed text (str)
    """
    model.eval()
    out_list = []
    
    # forward the prompt, compute prompt's ppl
    prompt_list = []
    char_prompt = list(prompt)
    for char in char_prompt:
        prompt_list.append(my_data.vocab.string_to_id[char])
    x = torch.tensor(prompt_list).to(DEVICE).unsqueeze(1)
    
    states = model.init_hidden(1)
    logits, states = model(x, states)
    probs = F.softmax(logits[-1], dim=-1)
        
    if sample:
        assert False, "Implement me!"
    else:
        max_p, ix = torch.topk(probs, k=1, dim=-1)

    out_list.append(my_data.vocab.id_to_string[int(ix)])
    x = ix.unsqueeze(1)
    
    # decode 
    for k in range(steps):
        logits, states = model(x, states)
        probs = F.softmax(logits, dim=-1)
        if sample:  # sample from the distribution or take the most likely
            assert False, "Implement me!"
        else:
            _, ix = torch.topk(probs, k=1, dim=-1)
        out_list.append(my_data.vocab.id_to_string[int(ix)])
        x = ix
    return ''.join(out_list)

#1.2.2
learning_rate = 0.0005
clipping = 1.0
embedding_size = 64
rnn_size = 2048
rnn_num_layers = 1

# vocab_size = len(module.vocab.itos)
vocab_size = len(my_data.vocab.id_to_string)
print(F"vocab size: {vocab_size}")

model = RNNModel(
    num_classes=vocab_size, emb_dim=embedding_size, hidden_dim=rnn_size,
    num_layers=rnn_num_layers)
model = model.to(DEVICE)
hidden = model.init_hidden(batch_size)

loss_fn = nn.CrossEntropyLoss(ignore_index=0)
optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)

"""## Training loop"""

# Training


ppl_epoch_list=[]

num_epochs = 30
report_every = 30
prompt = "Dogs like best to"

for ep in range(num_epochs):
    print(f"=== start epoch {ep} ===")
    state = model.init_hidden(batch_size)

    ppl_epoch=0
    for idx in range(len(batches)):
        batch = batches[idx]
        model.train()
        optimizer.zero_grad()
        state = custom_detach(state)
        
        input = batch[:-1]
        target = batch[1:].reshape(-1)

        bsz = input.shape[1]
        prev_bsz = state.shape[1]
        if bsz != prev_bsz:
            state = state[:, :bsz, :]
        output, state = model(input, state)
        loss = loss_fn(output, target)
        #2.1
        ppl=torch.exp(loss)
        ppl_epoch=ppl_epoch+ppl.item()


        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)

        optimizer.step()
        if idx % report_every == 0:
            print(f"train loss: {loss.item()}")  # replace me by the line below!
            print(f"train ppl: {ppl.item()}")
            generated_text = complete(model, prompt, 128, sample=False)
            print(f'----------------- epoch/batch {ep}/{idx} -----------------')
            print(prompt)
            print(generated_text)
            print(f'----------------- end generated text -------------------')

#2.2 Text generated by the model at three different stages
        if idx  == 0 and ep==4:
          generated_txt_beginning=complete(model, prompt, 128, sample=False)


        if idx == 40 and ep==15:
          generated_txt_middle=complete(model, prompt, 128, sample=False)


        if idx == 80 and ep==29:
          generated_txt_final=complete(model, prompt, 128, sample=False)

    ppl_epoch_list.append(ppl_epoch/len(batches))

#2.2
import matplotlib.pyplot as plt
#print(len(ppl_epoch_list), ppl_epoch_list[-1])
epochs=range(num_epochs)
#print(epochs)
plt.plot(epochs, ppl_epoch_list, marker="o", markerfacecolor="blue", markersize=4, color="blue", linewidth=2)
plt.title("Perplexity over Epochs, log scale")
plt.xlabel("Epochs")
plt.ylabel("Perplexity")
plt.yscale('log')

#2.2
print(prompt+generated_txt_beginning +"\n")

print(prompt+generated_txt_middle +"\n")

print(prompt+generated_txt_final +"\n")

#2.3
#A title of a fable which exists in the book.
input="MERCURY AND THE WOODMAN"
print(input+complete(model, input, 512, sample=False)+"\n"+"\n")

#A title which you invent, which is not in the book, but similar in the style
input="THE WOLF AND THE LION"
print(input+complete(model, input, 512, sample=False)+"\n"+"\n")

#Some texts in a similar style.
input="one day a cat was" 
print(input+complete(model, input, 512, sample=False)+"\n"+"\n")

#Anything you think might be interesting.
input="the team that will lift the football world cup will be"
print(input+complete(model, input, 512, sample=False))

#
#input="THE FOX"
#print(input+complete(model, input, 512, sample=False)+"\n"+"\n")

####LSTM
# LSTM based language model
class LSTMModel(nn.Module):

    def __init__(self, num_classes, emb_dim, hidden_dim, cell_dim, num_layers):
        """Parameters:
        
          num_classes (int): number of input/output classes
          emb_dim (int): token embedding size
          hidden_dim (int): hidden layer size of LSTMs
          #3.1
          cell_dim (int): cell layer size of LSTMs
          num_layers (int): number of LSTM layers
        """
        super().__init__()
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim
        #3.1
        self.cell_dim= cell_dim
        self.num_layers = num_layers

        self.input_layer = nn.Embedding(num_classes, emb_dim)
        

        ##here 3.1
        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hidden_dim, num_layers=num_layers)
        
 

        self.out_layer = nn.Linear(hidden_dim, num_classes)


    def forward(self, input, state):
        emb = self.input_layer(input)
        
        ##here 3.1
        output, state = self.lstm(emb, state)
        

        output = self.out_layer(output)
        output = output.view(-1, self.num_classes)
        return output, state

    def init_hidden(self, bsz):
        weight=next(self.parameters())
        return weight.new_zeros(self.num_layers, bsz, self.hidden_dim)
        
    ##3.1   
    def init_cell(self, bsz):
        weight=next(self.parameters())
        return weight.new_zeros(self.num_layers, bsz, self.cell_dim)
        
    


##
def custom_detach(h):
  return h.detach()

#DECODING

@torch.no_grad()
def complete(model, prompt, steps, sample=False):
    """Complete the prompt for as long as given steps using the model.
    
    Parameters:
      model: language model
      prompt (str): text segment to be completed
      steps (int): number of decoding steps.
      sample (bool): If True, sample from the model. Otherwise greedy.

    Returns:
      completed text (str)
    """
    model.eval()
    out_list = []
    
    # forward the prompt, compute prompt's ppl
    prompt_list = []
    char_prompt = list(prompt)
    for char in char_prompt:
        prompt_list.append(my_data.vocab.string_to_id[char])
    x = torch.tensor(prompt_list).to(DEVICE).unsqueeze(1)
    
    #3.1
    states = (model.init_hidden(1), model.init_cell(1)) 


    logits, states = model(x, states)
    probs = F.softmax(logits[-1], dim=-1)
        
    if sample:
        assert True, "Implement me!"
        #3.3
        ix = torch.multinomial(probs, num_samples=1)

    else:
        max_p, ix = torch.topk(probs, k=1, dim=-1)

    out_list.append(my_data.vocab.id_to_string[int(ix)])
    x = ix.unsqueeze(1)
    
    # decode 
    for k in range(steps):
        logits, states = model(x, states)
        probs = F.softmax(logits, dim=-1)
        if sample:  # sample from the distribution or take the most likely
            assert True, "Implement me!"
            #3.3
            ix = torch.multinomial(probs, num_samples=1)
            
        else:
            _, ix = torch.topk(probs, k=1, dim=-1)

        out_list.append(my_data.vocab.id_to_string[int(ix)])
        x = ix
    return ''.join(out_list)

#learning_rate = 0.0005
#3.2
learning_rate = 0.001
clipping = 1.0
embedding_size = 64
lstm_size = 2048
lstm_num_layers = 1

# vocab_size = len(module.vocab.itos)
vocab_size = len(my_data.vocab.id_to_string)
print(F"vocab size: {vocab_size}")
#3.1
model = LSTMModel(
    num_classes=vocab_size, emb_dim=embedding_size, hidden_dim=lstm_size, cell_dim=lstm_size,
    num_layers=lstm_num_layers)

model = model.to(DEVICE)

hidden = model.init_hidden(batch_size)
#3.1
cell= model.init_cell(batch_size)

loss_fn = nn.CrossEntropyLoss(ignore_index=0)
optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)

# Training

ppl_epoch_list_LSTM=[]


num_epochs = 30
report_every = 30
prompt = "Dogs like best to"

#3.5.1
#prompt = "There must be some kind of way outta here"

#3.5.2
#prompt = "Homer_Simpson: What's the matter, Moe?"

for ep in range(num_epochs):
    print(f"=== start epoch {ep} ===")
    hidden = model.init_hidden(batch_size)
    #3.1
    cell= model.init_cell(batch_size)
    state = (hidden, cell)
    
    
    ppl_epoch=0
    for idx in range(len(batches)):
      
        batch = batches[idx]
        model.train()
        optimizer.zero_grad()
        #3.1
        state = (custom_detach(state[0]), custom_detach(state[1]))
        
        input = batch[:-1]
        target = batch[1:].reshape(-1)

        bsz = input.shape[1]
   
        prev_bsz = state[0].shape[1]
        
        if bsz != prev_bsz:
            state = state[:, :bsz, :]
        output, state = model(input, state)
        loss = loss_fn(output, target)
        
        ppl=torch.exp(loss)
        ppl_epoch=ppl_epoch+ppl.item()

        loss.backward()
     
       
        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)

        optimizer.step()

        if idx % report_every == 0:

           print(f"train loss: {loss.item()}")  # replace me by the line below!
           print(f"train ppl: {ppl.item()}")
           #TRUE put Decoding
           generated_text = complete(model, prompt, 128, sample=False)
           print(f'----------------- epoch/batch {ep}/{idx} -----------------')
           print(prompt)
           print(generated_text)
           print(f'----------------- end generated text -------------------')

    
    ppl_epoch_list_LSTM.append(ppl_epoch/len(batches))

#print(len(ppl_epoch_list_LSTM),  ppl_epoch_list_LSTM[-1])
epochs=range(num_epochs)
#print(epochs)
plt.plot(epochs, ppl_epoch_list_LSTM, marker="o", markerfacecolor="blue", markersize=4, color="blue", linewidth=2)
plt.title("Perplexity over Epochs, log scale")
#plt.hlines(1.03, 0, epochs[-1]+1, linestyle="dashed",colors='black')
plt.xlabel("Epochs")
plt.ylabel("Perplexity")
plt.yscale('log')

#3.4 (i)
input="THE THREE TRADESMEN"
print(input+complete(model, input, 512, sample=False)+"\n"+"\n")

#3.4 (i)
input="THE THREE TRADESMEN"
print(input+complete(model, input, 512, sample=True)+"\n"+"\n")

#3.4 (ii)
input="THE SMART DOG"
print(input+complete(model, input, 512, sample=False)+"\n"+"\n")

#3.4 (ii)
input="THE SMART DOG"
print(input+complete(model, input, 512, sample=True)+"\n"+"\n")

#3.5.1
input="There is"
print(input+complete(model, input, 512, sample=False))

input="4th of December"
print(input+complete(model, input, 512, sample=False))

input="Why is"
print(input+complete(model, input, 512, sample=False))

input="Where are you going"
print(input+complete(model, input, 512, sample=False))

#3.5.2
input="Lenny_Leonard: Hey Moe how are you doing?"
print(input+complete(model, input, 512, sample=False))